## 基于istio链路跟踪的拓展思考 - 服务运营状态分析平台

### 摘要
微服务架构，相比于单体服务，可以做到灵活部署，资源与功能的独立，健壮性好等各种优势，目前已经成为了互联网时代的主流。  
在各公司的实践中，微服务架构下的服务数量将是以前的几十倍，服务部署阶段，若使用了kubernetes等容器调度平台，其容器数量可能是以前单体服务时代虚拟机数量的几百倍。  
因此，这种情况下，诊断与分析平台中的问题变的异常繁琐与艰难。  
在上述场景下，链路跟踪，应运而生，为每一次请求关联唯一的编号，traceId，以还原这一笔请求的所有参与方以及具体的元数据。但是当我们从架构以及业务视角去观察微服务架构的时候，发现单次链路跟踪好像不能带来什么帮助，无法提供数据化的指导, 然而两者之间却存在一定的因果关系。  
因此本篇论文将通过基于istio的链路跟踪，拓展思考到云原生时代的数字基座。  


### 说明
本篇论文所实现的方案基于以下平台

    基于华为云CCE与ASM产品

1. kubernetes 1.21
2. istio 1.13

### 拓展思考
现实生活中，我们经常乘坐地铁，从进站，换乘再到出站，一次乘坐就好比一次链路跟踪，地铁站记录了你经过的每一个站点以及每一个时刻。  

乘客愿意乘坐地铁，因为地铁的确降低了我们的通勤时间，四通八达的站点还能做到常年午休，价格低廉亲民，班次准时准点。
<div align="center">
<img src=images/passenger-per.png width="50%"/>
</div>

如果此刻，你的角色不是乘客，而是**负责地铁运营状态的管理人员**，你可能想知道，上海市地铁过去的每一时刻地铁所有站点的进出站人员数量，早晚高峰期在几点开始又在几点结束，重要节假日乘客量是多少等等，为优化地铁排班提供数据化引导，甚至可以将这些数据贡献给相关部门，哪里可以开商场，哪里需要拓宽道路等等。  

再让我们切换下角色，此刻你是**地铁建设的评估人员**，地铁站点设置的是否合理，早晚高峰期，站点是否能平稳度过，不出现过于拥堵的情况，偏僻位置的站点，客流量是否足够等等，这一切的数据反馈出在地铁前期规划时各方的判断与规划是否合理。  

最后一个角色，**地铁建设的规划人员**，当我们要开发新的线路时，延伸地铁站点时，是否有足够的数据来支撑我们做的每一个决定，地铁是一个民生工程，成果的好坏将直接影响城市的发展进度。  
<div align="center">
<img src=images/地铁运营思考.jpg width="90%"/>
</div>


每一个角色看似孤立，实际上却有着因果关系。管理人员所依赖的宏观数据，正是由若干个乘客的数据所组成，若干个乘客的数据形成了宏观数据。

### 云原生数字基座

云原生时代，数据量爆发式增长，但大家对各自的服务的数字化要求却有很多共同点。平台正在发生什么，未来可能要发生什么，过去的投入是否达到了预期等等。
上个时代，我们要跨越好几个团队，花费大量资源才能大概知道上述的几个问题，而且这一能力很难下沉到基建，各个团队不同的数据格式，不同的标准。


下面列出我们团队关注的几点，

#### 从宏观到微观的角度

1. 时间段内有哪些服务被访问 - 如每个服务的每个版本每天被访问了多少次数，加大或者调优对某些服务的投入
2. 服务有哪些api被请求 - 如服务每天被请求了一百万次，精确到api级要定位到被访问的api分布，提高对用户的响应质量与加大特定功能的投入
3. 服务请求了哪些第三方服务 - 如服务调用了哪些第三方服务以及这些服务的响应时长分布，如过去一天调用了服务A一万次，其中百分之八十的响应区间是在200-500ms内，
4. 服务间访问依赖关系 - 如服务间访问关系拓扑图，找出其中的隐患
5. 服务进出事件的完整记录 - 如每个服务的每个版本在其生命周期内的完整的进出事件都能完整记录，当服务出现异常时，能还原那一刻的具体事件
6. 记录服务进出的元数据 - 如请求头/体，响应头/体，完整复现参与方以及具体的数据


#### 从角色类型的维度

- 开发团队
  1. 单次请求的链路
  2. 请求链路的数据复现 - 请求头/体, 响应头/体
  3. 根据traceid能定位到具体的日志

- 架构师：
  1. 各服务响应请求的宏观数据，如每天被请求的api的响应分布，哪些服务链路过长等
  2. 分析出平台的瓶颈 - 哪些api响应过慢，哪些服务报错过多
  3. 分析出平台架构设计的缺陷 - 如哪些服务应该被拆分，哪些应该聚合等

- 业务团队：
  1. 服务宏观被调用的分布，哪些服务用户使用的多，新上线的功能时候被用户使用等
  2. 通过查看用户访问数据，确定哪些项目应该多投入人力物力
  3. 通过历史的时序数据预测到未来的指标增长

- DevOps团队：
  1. 平台应该适用于任何工程，独立于特定语言，做到几乎无需任何代码层的改动 - 无需引入第三方代码层组件
  2. 平台的功能迭代应独立于工程，用户侧无感知
  3. 服务间的各种访问元数据应该能独立存储


上述是我们列出的几个主要关注点，在云原生时代，实现这一切应该几乎不需要任何代码修改，不依赖任何特定框架，不依赖任何特定语言，成为数字基建，赋能给每一个团队。


### 方案调研

#### 传统方案
    传统方案在代码层引用第三方开源或者商业化的组件，在代码层将数据发送到中央平台，中央平台负责数据的管理与展示

传统方案有以下几个问题:
1. 代码层组件，需要重新编译打包发布，工作量巨大
2. 代码层组件，风险性大，一旦出现问题，很难定位到具体原因
3. 代码层组件，一般只能适用于特定几个语言，如python，java等，对于很多新语言支持较慢
4. 代码层组件一旦更新，所有使用到改组件的工程都要重新打包编译

#### 云原生方案 - [Istio](https://istio.io/)
    云原生方案，社区提出了服务网格的概念，该领域的领导者，Istio，在基础设施层解决了大家关注的各种个问题。
    Istio，服务网格，使用单pod单sidecar模式，劫持pod的进出流量，独立管理面与数据面，将各种数据通过sidecar提交给后台。

服务网格解决方案有以下几个显著优点:
1. 通过sidecar流量劫持，使得业务层代码只需关注业务逻辑，简化代码架构，降低故障发生
2. sidecar是基于协议层的流量劫持，不关注具体语言
3. 代码几乎无改动即可实现链路跟踪等功能
4. sidecar的span中包含了众多元数据，便于后期分析
5. 最重要的一点，istio将能力下放到了底层，赋能基建



### 平台功能介绍
现在，让我们以四个项目来演示平台功能。

- istio-tracing-bill
- istio-tracing-mall
- istio-tracing-order
- istio-tracing-payment

这四个项目分别使用不同的编程语言，go，python等，在代码上无任何共性。

#### 宏观

- ##### 概览饼状图
<div align="center">
<img src=images/overview.png width="80%"/>
</div>

上图阐述了某时间范围内，所有服务被访问的情况。  
左边代表的是从外部入口进入集群的访问次数，下述两侧为被访问的方法以及响应码的分布饼状图.  
右侧代表在内部被访问的次数，以及两侧为被访问的方法以及响应码的分布饼状图

- ##### 被访问的api分布饼状图
当我们知道了有哪些服务被访问，下面我们想知道具体的api被访问的分布
<div align="center">
<img src=images/api-share.png width="80%"/>
</div>

上图阐述了某时间范围内，具体服务的具体版本被访问的api分布图。  
**从开发人员的角度，api被调用了就是api被访问了，而从业务人员的角度，api被访问，就是某些服务的具体功能被请求了，通过这一张图，再配合具体的说明，可以定位到每一个服务有哪些功能被使用**

- ##### 客户端视角api请求概况
上图是从服务端角度来看分布，我们有时候想知道一个具体服务的某些版本访问了哪些外部服务以及情况分布
<div align="center">
<img src=images/api-share-client-side.png width="80%"/>
</div>
上述阐述了某时间范围内，istio-tracing-order服务的build_174606版本访问了hd-demo下的istio-tracing-paymeng服务,以及请求服务的api分布，状态码响应分布

- ##### api被访问增长情况
知道了服务被请求的api分布，这个时候我们想知道具体api被请求的时序变化，如增长量变化等
<div align="center">
<img src=images/api-request-growth.png width="80%"/>
</div>
根据上图，我们可以知道具体功能被使用的时序分布情况

- ##### 依赖关系图

从宏观视角观察服务间的依赖关系（具体服务的具体版本的进出状况）
<div align="center">
<img src=images/dependency.png width="80%"/>
</div>
通过上图，我们可以看到istio-tracing-order的build_174606版本访问的stio-tracing-payment服务（之间的访问次数, 甚至可以定位到目标服务的逻辑版本）

- ##### 服务响应与请求时长分布
我们知道了概览，服务被访问的情况以及服务间调用的依赖图，但是质量如何去衡量，希望有一个宏观但也能反应服务响应质量的图。
<div align="center">
<img src=images/overview-duraton.png width="80%"/>
</div>

上图我们以istio-tracing-mall为例，该服务的build_174602版本

- 第一行的左侧，SERVER的分布图，阐述了istio-tracing-mall作为服务端，响应请求的响应时间分布图
- 第一行的右侧，CLIENT的分布图，阐述了istio-tracing-mall作为客户端，请求外部服务，外服服务的响应时长分布图
- 中间的表格，SERVER，说明作为服务端，被请求的api概况，根据uri分组统计，访问次数，响应时长以及具体版本等
- 最底端的表格， CLIENT，说明作为客户端去请求外服服务的api情况，以及访问次数，响应时长以及具体版本等

#### 微观

- ##### 请求记录

开发者一般不太关心这些宏观数据，更想根据我的服务上线后具体api的响应情况，能查看元数据，如响应码，响应时间等。
<div align="center">
<img src=images/activities-server.png width="80%"/>
</div>

<div align="center">
<img src=images/activities-client.png width="80%"/>
</div>
上述两张图，分别为作为服务端与客户端最近100次请求详情

功能介绍

- 顶层的下拉列表，可供选择的有具体项目，具体版本，pod，类型，状态码，请求时长，uri等，多维度查看符合条件的请求
- 表格字段，第一列为本次请求的traceid(**可链接到链路跟踪详情图**)，uri，持续时间，响应码，响应数据量，请求数据量，时间戳等，深度还原这一次的情况

- ##### 链路跟踪
当我们点击上图的traceid，将跳转到链路跟踪图，还原这一次请求的详情

<div align="center">
<img src=images/trace-detail.png width="80%"/>
</div>

#### 单次链路跟踪(拓展)
单次的链路跟踪还原，让我们知道了那一可以的访问详情，但这一切好像只是定位了问题的发生主体，未能具体告诉用户这一次的关联数据

- 服务间访问的具体数据是什么，如istio-tracing-mall去访问istio-tracing-payment服务，具体的请求头请求体，响应头响应体是什么
  以前我们通过在日志里打印具体的元数据，虽然为排查带来了帮助，但是加大了日志量，日志变的不再清晰，日志应该更多的展示业务层的情况
- 日志关联性 - 某一个时间段内，我们处理了大量的请求，如果这一笔请求有问题，我们必须定位到具体的服务，然后在日志里根据traceid去搜索排查，特别麻烦
  如istio-trace-mall处理本次请求时，我所产生的日志有哪些

功能上传统方案，在日志里答应，然后通过elasticsearch等工具去搜索定位也能解决，但是耗时耗力，如果时间维度跨服比较大，如一笔请求是三个月前的，传统elasticsearch的方案就变的麻烦  

得益于istio的[EnvoyFilter](https://istio.io/latest/docs/reference/config/networking/envoy-filter/)功能，我们可以捕获pod的进出，将这些元数据入库便于后期的定位。

<div align="center">
<img src=images/trace-detail-with-link.png width="80%"/>
</div>
如果该服务是服务发起方，则会有请求头/体与响应头/体的按钮，点击按钮将跳转到页面，展示了具体的请求与响应数据.
<div align="center">
<img src=images/request-data.png width="80%"/>
</div><div align="center">
<img src=images/response-data.png width="80%"/>
</div>

上述两张图分别为请求头/体与响应头/体的详情。

### 总结

上述功能点，可以从各位维度去观察去统计去定位平台中的各个问题与指标，而且这一切不依赖任何特定语言与框架，将这一切直接赋能到基建。  

基于我们拥有的数据以及上述功能，下面我们列出了几点探索

1. 基于历史的用户访问，是否可以分析出未来的趋势
2. 基于服务关系图，是否可以分析出架构设计的优劣性，对架构设计提出评估
3. 基于服务进出的元数据实现业务场景 - [CloudEvents](https://cloudevents.io/)给了我们很好的借鉴，通过统一平台的数据格式，是否可以通过分析这些元数据实现业务价值，如用户在我们各平台的请求情况
4. 基于服务进出的元数据实现机器学习 - 大部分场景下的机器学习是从数据库中抽取数据，链路过长，跨部门协作等，是否可以直接通过分析元数据实现部分场景的机器学习

